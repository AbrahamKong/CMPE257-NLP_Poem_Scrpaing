{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPPoemScraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrahamKong/CMPE257-NLP_Poem_Scrpaing/blob/main/NLPPoemScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "NlEE2dkPaI8H"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqIRrHqpBVYw",
        "outputId": "6ddd492d-0780-4bef-8c6f-cc0af2e2600b"
      },
      "source": [
        "pip install beautifulsoup4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqfiv4fZQu_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ee9a4c-e82b-4ae8-c427-82764f3a739d"
      },
      "source": [
        "!pip install -U pandas-profiling"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas-profiling in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Collecting pandas-profiling\n",
            "  Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB)\n",
            "\u001b[K     |████████████████████████████████| 261 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (3.2.2)\n",
            "Collecting joblib~=1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 10.9 MB/s \n",
            "\u001b[?25hCollecting pydantic>=1.8.1\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 13.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.5.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.4.1)\n",
            "Collecting tangled-up-in-unicode==0.1.0\n",
            "  Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 28.2 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.0.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: markupsafe~=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.0.1)\n",
            "Collecting multimethod>=1.4\n",
            "  Downloading multimethod-1.8-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.11.3)\n",
            "Collecting visions[type_image_path]==0.7.4\n",
            "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting phik>=0.11.1\n",
            "  Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB)\n",
            "\u001b[K     |████████████████████████████████| 690 kB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (4.63.0)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.21.5)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (21.4.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (2.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (7.1.2)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.0->pandas-profiling) (3.10.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2018.9)\n",
            "Collecting scipy>=1.4.1\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas-profiling) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (1.24.3)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling) (1.3.0)\n",
            "Building wheels for collected packages: htmlmin, imagehash\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=09e7fa3c76d9105818dace5b9e4a535cc3dd4960ac48b0531e09057197371bf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=da758da2b1c89f1ebac0f2543cecc42b43ff557839810de526197958db0e7ac1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n",
            "Successfully built htmlmin imagehash\n",
            "Installing collected packages: tangled-up-in-unicode, scipy, multimethod, visions, joblib, imagehash, requests, PyYAML, pydantic, phik, htmlmin, pandas-profiling\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 htmlmin-0.1.12 imagehash-4.2.1 joblib-1.0.1 multimethod-1.8 pandas-profiling-3.1.0 phik-0.12.2 pydantic-1.9.0 requests-2.27.1 scipy-1.7.3 tangled-up-in-unicode-0.1.0 visions-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Rest"
      ],
      "metadata": {
        "id": "spUMYFRpaMmU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJGmwTWRCdyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be44959a-0536-4b09-f9ea-18c0bbb821ef"
      },
      "source": [
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bs4 as bs\n",
        "import re\n",
        "\n",
        "john_keats_poems = \"/content/drive/MyDrive/DataSets/john_keats.csv\"\n",
        "lord_tennyson_poems = \"/content/drive/MyDrive/DataSets/alfred_tennyson.csv\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQI8xj9_CfOb"
      },
      "source": [
        "def beautify_txt(text):\n",
        "    # final_data = (((text).replace(u'\\xa0', u' ')).replace(r'\\r',u'\\n').replace(r'\\n', ' '))\n",
        "    final_data = (((text).replace(u'\\xa0', u' ')).replace(r'\\r',u'\\n'))\n",
        "\n",
        "    return final_data\n",
        "\n",
        "Alfred_Lord_Tennyson_Poems=[\"https://allpoetry.com/poem/8473255-A-Farewell-by-Alfred-Lord-Tennyson\",\n",
        "\"https://allpoetry.com/poem/8473237-After-Thought-by-Alfred-Lord-Tennyson\",\n",
        "\"https://allpoetry.com/All-Things-will-Die\",\n",
        "\"https://allpoetry.com/poem/8473177-Amphion-by-Alfred-Lord-Tennyson\",\n",
        "\"https://allpoetry.com/Ask-Me-No-More\",\n",
        "\"https://allpoetry.com/Audley-Court\",\n",
        "\"https://allpoetry.com/Battle-Of-Brunanburgh\",\n",
        "\"https://allpoetry.com/Beautiful-City\",\n",
        "\"https://allpoetry.com/Blow,-Bugle,-Blow\",\n",
        "\"https://allpoetry.com/poem/8473297-Boadicea-by-Alfred-Lord-Tennyson\"]\n",
        "\n",
        "John_Keats_Poems=[\n",
        "\"https://allpoetry.com/A-Draught-Of-Sunshine\",\n",
        "\"https://allpoetry.com/A-Galloway-Song\",\n",
        "\"https://allpoetry.com/A-Party-Of-Lovers\",\n",
        "\"https://allpoetry.com/A-Prophecy:-To-George-Keats-In-America\",\n",
        "\"https://allpoetry.com/A-Song-About-Myself\",\n",
        "\"https://allpoetry.com/A-Thing-Of-Beauty-\",\n",
        "\"https://allpoetry.com/Acrostic-:-Georgiana-Augusta-Keats\",\n",
        "\"https://allpoetry.com/An-Extempore\",\n",
        "\"https://allpoetry.com/Answer-To-A-Sonnet-By-J.H.Reynolds\",\n",
        "\"https://allpoetry.com/Apollo-And-The-Graces\"]    \n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a sample of defensive programming; attempt to read the scraped data file. If it does not exist, scrape the data; if it does, read it in."
      ],
      "metadata": {
        "id": "ILbxvT2i5fTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fxqgcXkdVcTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TX27GdKh5dkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#------------------------------------------\n",
        "# try to get the poem, if it is there, load it \n",
        "# if not scarpe it into a df. #playdefense\n",
        "#\n",
        "#------------------------------------------\n",
        "def get_poems(poets_poems):\n",
        "  file_exists = os.path.exists(poets_poems)\n",
        "  print(f\"{poets_poems} file_exists\")\n",
        "  if (file_exists):\n",
        "    #load it!\n",
        "    df = pd.read_csv(poets_poems, sep=';')\n",
        "    print(df.head(2))\n",
        "    return df\n",
        "  else:\n",
        "    print_the_poem(poets_poems); # call code to scrape\n"
      ],
      "metadata": {
        "id": "3aTGAo7rMvjN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALT_DF = get_poems(lord_tennyson_poems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "YokVHukOcypt",
        "outputId": "1c876d3e-cd15-4f83-abab-9714fef9ecbe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DataSets/alfred_tennyson.csv file_exists\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8b7edbca586a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mALT_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_poems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlord_tennyson_poems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-1c37d55c45b2>\u001b[0m in \u001b[0;36mget_poems\u001b[0;34m(poets_poems)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprint_the_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoets_poems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m# call code to scrape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-d412145fced7>\u001b[0m in \u001b[0;36mprint_the_poem\u001b[0;34m(poet_df)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m#add iterrows to print all poems 04-8-2022 11:56 pm!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mpoem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoet_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Poem'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{line}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALT_DF.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "CdoEv_e9cNoE",
        "outputId": "43e053d7-c4c1-4dae-8ca1-850affd2c6ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3ec966ac69d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mALT_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'type'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALT_DF['Poem'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "Jz5MrLB6O9Zy",
        "outputId": "675639c7-8182-4792-dae4-54276c591086"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1e16fcb65be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mALT_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Poem'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample function to print out a poem . Make sure your outputs of poems esp when you juxtapose the POS from different poets, are in human readable format!"
      ],
      "metadata": {
        "id": "-lseCrbN6mlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_the_poem(poet_df):\n",
        "  #add iterrows to print all poems 04-8-2022 11:56 pm!\n",
        "  \n",
        "  poem = poet_df['Poem'][0].split('\\n')\n",
        "  for line in poem:\n",
        "    print(f\"{line}\")"
      ],
      "metadata": {
        "id": "2Sfv1pQ8PhEt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_the_poem(ALT_DF)"
      ],
      "metadata": {
        "id": "NNc2nCh7cfbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poems_results_ALT=[]\n",
        "def scrapeALTPoetsPoems():\n",
        "  for url in Alfred_Lord_Tennyson_Poems:\n",
        "    header = {'User-Agent':'Mozilla/5.0'}\n",
        "    requests = urllib.request.Request(url,headers=header)\n",
        "    requestread = urllib.request.urlopen(requests).read()\n",
        "    soup = bs.BeautifulSoup(requestread,'html.parser')\n",
        "\n",
        "    poem_content = (beautify_txt(soup.find_all('div', class_=\"poem_body\")[0].text))\n",
        "   \n",
        "    poem_title = soup.find_all('h1')[0].text\n",
        "    poems_results_ALT.append([poem_title, poem_content])\n",
        "  return poems_results_ALT"
      ],
      "metadata": {
        "id": "2oOQs-puMs6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFdfgpsvSPWR"
      },
      "source": [
        "poems_results_JK=[]\n",
        "def scrapeJKPoetsPoems():\n",
        "  for url in John_Keats_Poems:\n",
        "    print(url)\n",
        "    header = {'User-Agent':'Mozilla/5.0'}\n",
        "    requests = urllib.request.Request(url,headers=header)\n",
        "    requestread = urllib.request.urlopen(requests).read()\n",
        "    soup = bs.BeautifulSoup(requestread,'html.parser')\n",
        "    \n",
        "    poem_content = (beautify_txt(soup.find_all('div', class_=\"poem_body\")[0].text))\n",
        "   \n",
        "    poem_title = soup.find_all('h1')[0].text\n",
        "    poems_results_JK.append([poem_title, poem_content])\n",
        "  return poems_results_JK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl0Ai6zWRq7T"
      },
      "source": [
        "scrapeJKPoetsPoems = scrapeJKPoetsPoems()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qquq9utTXWD"
      },
      "source": [
        "JK_DF = pd.DataFrame(scrapeJKPoetsPoems)\n",
        "JK_DF.columns = ['Title', 'Poem']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjXc2Im3Tevc"
      },
      "source": [
        "JK_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JK_DF['Poem'][4]"
      ],
      "metadata": {
        "id": "_Rw9g2lLwjO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JK_DF['Poem'][6]"
      ],
      "metadata": {
        "id": "FfNPXBkemLFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KuaWqCWbmVs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_QT1YCUnjCO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_qCHTktThzG"
      },
      "source": [
        "JK_DF.to_csv(\"/content/drive/MyDrive/DataSets/john_keats.csv\", sep=';', index=\"False\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDyonNcaCx0I"
      },
      "source": [
        "ALT_data=scrapeALTPoetsPoems()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSvjS94cIVfU"
      },
      "source": [
        "ALT_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz4dq9rMI9SC"
      },
      "source": [
        "ALT_DF = pd.DataFrame(ALT_data)\n",
        "ALT_DF.columns = ['Title', 'Poem']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkPTih1ON5Vh"
      },
      "source": [
        "ALT_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LCl8z4kN59y"
      },
      "source": [
        "ALT_DF.to_csv(\"/content/drive/MyDrive/DataSets/alfred_tennyson.csv\", sep=\";\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ9MFmZNuCY0"
      },
      "source": [
        "df_john = pd.read_csv('/content/drive/MyDrive/DataSets/john_keats.csv', sep=';')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB3gwOf0uJFU"
      },
      "source": [
        "df_john"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KFNxXnlbQZf"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5LAGGHcSjhE"
      },
      "source": [
        "def cleanPoemText(text):\n",
        "    \n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"can not\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "\n",
        "    if embedding is not 'BERT':\n",
        "        text = re.sub(r\"[0-9]+\", ' ', text)\n",
        "        text = re.sub(r\"-\", ' ', text)\n",
        "    \n",
        "    \n",
        "    text = text.strip().lower()\n",
        "    \n",
        "    if embedding is 'WORD2VEC_NO_STOP':\n",
        "        # Removal of Stop words\n",
        "        default_stop_words = set(stopwords.words('english'))\n",
        "        default_stop_words.difference_update({'no', 'not', 'nor', 'too', 'any'})\n",
        "        stop_words = default_stop_words.union({\"'m\", \"n't\", \"'d\", \"'re\", \"'s\",\n",
        "                                               'would','must',\"'ve\",\"'ll\",'may'})\n",
        "    \n",
        "        word_list = word_tokenize(text)\n",
        "        filtered_list = [w for w in word_list if not w in stop_words]\n",
        "        text = ' '.join(filtered_list)\n",
        "    \n",
        "    if embedding is not 'BERT':\n",
        "        # Removal of other contractions\n",
        "        text = re.sub(r\"'\", ' ', text)\n",
        "    \n",
        "    # Replace punctuations with space\n",
        "    if embedding is 'BERT': # save ! ? . for end of the sentence detection [,/():;']\n",
        "        filters='\"#$%&*+<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "        text = re.sub(r'\\!+', '!', text)\n",
        "        text = re.sub(r'\\?+', '?', text)\n",
        "    else:\n",
        "        filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    translate_dict = dict((i, \" \") for i in filters)\n",
        "    translate_map = str.maketrans(translate_dict)\n",
        "    text = text.translate(translate_map)\n",
        "    \n",
        "    if embedding is 'BERT':\n",
        "        text = re.sub(r'\\( *\\)', ' ', text)\n",
        "\n",
        "    if embedding is not 'BERT':\n",
        "        text = ' '.join([w for w in text.split() if len(w)>1])\n",
        "\n",
        "    # Replace multiple space with one space\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    \n",
        "    text = ''.join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy2v10sXeiLN"
      },
      "source": [
        "def normalization_pos(text):\n",
        "    # Lemmatization and Stemming according to POS tags\n",
        "\n",
        "    word_list = word_tokenize(text)\n",
        "    rev = []\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    stemmer = PorterStemmer() \n",
        "    for word, tag in pos_tag(word_list):\n",
        "        if tag.startswith('J'):\n",
        "            w = lemmatizer.lemmatize(word, pos='a')\n",
        "        elif tag.startswith('V'):\n",
        "            w = lemmatizer.lemmatize(word, pos='v')\n",
        "        elif tag.startswith('N'):\n",
        "            w = lemmatizer.lemmatize(word, pos='n')\n",
        "        elif tag.startswith('R'):\n",
        "            w = lemmatizer.lemmatize(word, pos='r')\n",
        "        else:\n",
        "            w = word\n",
        "        w = stemmer.stem(w)\n",
        "        rev.append(w)\n",
        "    review = ' '.join(rev)\n",
        "    return review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u978rv6ne2w3"
      },
      "source": [
        "!pip install bert-embedding\n",
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p98w-uMnexM5"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from bert_embedding import BertEmbedding\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "#Glove, COVE, FastText and other langauge embeddings "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SX0IQPHcVL6"
      },
      "source": [
        "embedding = ''\n",
        "# for Word2Vec with stop words\n",
        "ALT_DF['Poem'] = ALT_DF['Poem'].apply(cleanPoemText)\n",
        "ALT_DF.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KliNPiiZcdvU"
      },
      "source": [
        "ALT_DF['clean_text_normalized'] = ALT_DF['Poem'].apply(normalization_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLqvntKreo-E"
      },
      "source": [
        "ALT_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save everyting to disk!!!"
      ],
      "metadata": {
        "id": "xs2mZKAZWn_B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4qxqyNmtm2Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}